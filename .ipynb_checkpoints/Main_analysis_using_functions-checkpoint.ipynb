{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c59cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aea5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import statistics\n",
    "import scipy as sp\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "import math\n",
    "import geopy.distance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids # or from sklearn.cluster\n",
    "from tslearn.metrics import dtw\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ba533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for basemaps\n",
    "import contextily as ctx\n",
    "\n",
    "# For spatial statistics\n",
    "import esda\n",
    "from esda.moran import Moran, Moran_Local\n",
    "\n",
    "import splot\n",
    "from splot.esda import moran_scatterplot, plot_moran, lisa_cluster,plot_moran_simulation\n",
    "\n",
    "import libpysal as lps\n",
    "\n",
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d707d",
   "metadata": {},
   "source": [
    "# Analysis using cities as initial divisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e449d",
   "metadata": {},
   "source": [
    "## read necessary files\n",
    "## coordinates of the regions (cities, microregions, etc)\n",
    "## shapefile of the regions\n",
    "\n",
    "### coordinates is a n by 2 array where the i'th entry is the coordinates of the i'th city [x, y]\n",
    "### adjacency is a n by n array where the ij'th entry is the weight of the vector from city i to city j\n",
    "###  both of coordinates and adjacency are given as numpy arrays\n",
    "### method should be 'sum' or 'mean'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425536bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read shapefile of the cities\n",
    "gdf_cities= gpd.read_file('polygon_shapefiles/cities/31MUE250GC_SIR.shp')\n",
    "#make a column including region number\n",
    "gdf_cities['city_index'] = gdf_cities.index\n",
    "\n",
    "#make the united boundary of the whole region using one of the shapfile\n",
    "gdf_boundary= gdf_cities.dissolve()\n",
    "\n",
    "### coordinates is a n by 2 array where the i'th entry is the coordinates of the i'th city [x, y]\n",
    "with open(\"Coordinates/Coordinates_cities.pkl\", \"rb\") as f:\n",
    "    coordinates_cities= pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7afb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/sima/flow_papers_from_thesis/Coordinates_cities.pkl\", \"rb\") as f:\n",
    "    coordinates_cities= pickle.load(f)\n",
    "    \n",
    "    #boundry dataset\n",
    "gdf= gpd.read_file('C:\\sima\\PhD\\Thesis\\Mg_dataset/31MUE250GC_SIR.shp')\n",
    "#gdf= gpd.read_file(r'C:\\sima\\PhD\\Brazilian_Data\\mg_municipios\\MG_total_boundry-shapefile\\th878nx5786.shp')\n",
    "gdf_b= gpd.read_file(r'C:\\sima\\PhD\\Brazilian_Data\\mg_municipios\\MG_total_boundry-shapefile\\th878nx5786.shp')\n",
    "points_list= find_n_points_on_boundry(gdf_b, n=100)\n",
    "\n",
    "dg = gpd.read_file(r'C:\\sima\\PhD\\Thesis\\Mg_dataset/31MUE250GC_SIR.shp')\n",
    "dg['city_num'] = dg.index  # Assign city index\n",
    "#boundary datset\n",
    "#gdf_b= "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54667e02",
   "metadata": {},
   "source": [
    "## Paramethers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9bc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MW = 12 #number of months\n",
    "mw = 12\n",
    "number_cities= len(gdf_cities)\n",
    "\n",
    "years=['2013', '2014', '2015', '2016']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5af86d",
   "metadata": {},
   "source": [
    "## Generate vector fields \n",
    "### for defined unites (cities) from OD matrices and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f54968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "%run Generate_vector_fields.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a736bba",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'OD_matrices/OD_matrices_cities/OD_2013_1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m vectors_flow_map[y]\u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOD_matrices/OD_matrices_cities/OD_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m         adj_matrix \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      9\u001b[0m         methods\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OD_matrices/OD_matrices_cities/OD_2013_1.pkl'"
     ]
    }
   ],
   "source": [
    "vectors_flow_map_cities={}\n",
    "for y in years:\n",
    "    vectors_flow_map_cities[y]= {}\n",
    "    for m in range(2):\n",
    "        with open( \"OD_matrices/OD_matrices_cities/OD_{}\".format(y) + \"_{}.pkl\".format(m+1), \"rb\") as f:\n",
    "            adj_matrix = pickle.load(f)\n",
    "            \n",
    "            \n",
    "            methods= 'mean'\n",
    "            vecs= generate_vector_for_defined_regions(adj_matrix, gdf_boundary, coordinates_cities, method)\n",
    "            \n",
    "            vectors_flow_map_cities[y][m+1]= vecs\n",
    "            \n",
    "            #with open(\"generated_vector_fields/flow_map_cities_{}\".format(y) + \"_{}.pkl\".format(m+1), \"wb\") as f:\n",
    "                #pickle.dump(vecs, f)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24071a71",
   "metadata": {},
   "source": [
    "## read the vector fileds that have been generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_flow_map_cities= {}\n",
    "for y in years:\n",
    "    vectors_flow_map_cities[y]= {}\n",
    "    for m in range(mw):\n",
    "        with open(\"generated_vector_fields/flow_map_cities_{}\".format(y) + \"_{}.pkl\".format(m+1), \"rb\") as f:\n",
    "            vecs = pickle.load(f)\n",
    "            vectors_flow_map_cities[y][m+1]= vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc745fa",
   "metadata": {},
   "source": [
    "## Visualise vector fields on map - Flow map visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df9ab9",
   "metadata": {},
   "source": [
    "#### plot vectors with different length and same color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_one_month= vectors_flow_map_cities['2013'][1]\n",
    "\n",
    "X = pd.DataFrame(coordinates_cities)[0].values\n",
    "Y = pd.DataFrame(coordinates_cities)[1].values\n",
    "\n",
    "U = [x[0]/3 for x in vector_one_month]\n",
    "V = [x[1]/3 for x in vector_one_month]\n",
    "\n",
    "#diff_ = len(X)-len(U)\n",
    "#for _ in range(diff_):\n",
    "   # U.append(0)\n",
    "    #V.append(0)\n",
    "\n",
    "print(len(X),len(Y),len(U),len(V))\n",
    "\n",
    "# create the quiver plot\n",
    "fig, ax  = plt.subplots(figsize=(6, 6), dpi=200)\n",
    "\n",
    "gdf_cities.plot(ax=ax, edgecolor='grey', color='None')\n",
    "ax.quiver(X, Y, U, V, scale=2, width=500, units='xy', color='orange')\n",
    "# ax.gca().set_aspect('equal')\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Vector Field')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc042eb",
   "metadata": {},
   "source": [
    "##### plot vectors with the same length and different colors where colors demonstrates the vector size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bb52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e7e44d",
   "metadata": {},
   "source": [
    "## Entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "%run entropy_calculation_function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_dictionary_cities = calculate_entropy(vectors_flow_map_cities, years, number_cities, mw=MW, n_label=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2da579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot maps with entropy values\n",
    "\n",
    "\n",
    "# **Find global min and max values for consistent color scaling**\n",
    "global_min = min(np.min(arr) for arr in entropy_dictionary_cities.values())\n",
    "global_max = max(np.max(arr) for arr in entropy_dictionary_cities.values())\n",
    "\n",
    "# Define colormap\n",
    "orig_map = plt.cm.get_cmap('RdBu').reversed()  # Reverse color map\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(1, len(years), figsize=(80, 20))\n",
    "\n",
    "# Loop through years and plot each map\n",
    "for i, y in enumerate(years):\n",
    "    gdf_cities[f'entropy_{y}'] = gdf_cities.apply(lambda row: add_ent(row, entropy_dictionary_cities[y]), axis=1)\n",
    "\n",
    "    ax = axes[i] if len(years) > 1 else axes  # Handle single subplot case\n",
    "\n",
    "    img = gdf_cities.plot(column=f'entropy_{y}', cmap=orig_map, edgecolor='black', linewidth=0.1,\n",
    "                  ax=ax, legend=False, vmin=global_min, vmax=global_max)  # Apply global color scale\n",
    "\n",
    "    #ax.set_title('{}'.format(y))\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "\n",
    "# **Add a shared colorbar outside the figure**\n",
    "fig.subplots_adjust(right=0.85)  # Adjust space for colorbar\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])  # Position: [left, bottom, width, height]\n",
    "sm = plt.cm.ScalarMappable(cmap=orig_map, norm=plt.Normalize(vmin=global_min, vmax=global_max))\n",
    "fig.colorbar(sm, cax=cbar_ax, label=\"Entropy Value\")\n",
    "#plt.savefig('entropy_months_all_cbar_mean.jpg', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca99502",
   "metadata": {},
   "source": [
    "## calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "%run Cosine_similarity_function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import necessary libraries\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from tslearn.metrics import dtw\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all city vectors in order across all years\n",
    "vector_city_months_list = []\n",
    "\n",
    "for ye in years:\n",
    "    vector_city_months_list.append(np.stack(list(vectors_flow_map_cities[ye].values()), axis=-1))\n",
    "\n",
    "# Concatenate along the month axis to create a (number_regions, 2, total_months) array\n",
    "vectors_city_months = np.concatenate(vector_city_months_list, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa59588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cosine similarity across the full 4-year period\n",
    "\n",
    "vector_months_city_cosine_similarity = calculate_cosine_sim(vectors_city_months)\n",
    "\n",
    "\n",
    "#calculate the distances between the cosine similarity values for the months for the regions\n",
    "distance_matrix= calculate_dtw_distance_matrix(vector_months_city_cosine_similarity)\n",
    "\n",
    "#cluster the regions(cities) based on the distance matrix\n",
    "#select how many clusters you are interested to get\n",
    "n_clusters=5\n",
    "clusters, kmedoids= perform_kmedoids_clustering(distance_matrix, n_clusters)\n",
    "\n",
    "#calculate the mean of the cosine similarity values of the regions(cities) in each clusters\n",
    "clusters_mean= calculate_cluster_means(vector_months_city_cosine_similarity, clusters)\n",
    "\n",
    "#plot clusters mean cosine similarity values for months and fit a curve\n",
    "#assign color for clusters\n",
    "cluster_colors = {\n",
    "    0: \"#0072b2\",\n",
    "    1: \"#009e73\",\n",
    "    2: \"#e69f00\",\n",
    "    3: '#d55e00',\n",
    "    4:'#cc79a7'\n",
    "}\n",
    "\n",
    "plot_combined_cluster_trends(clusters_mean, cluster_colors, save_path_svg=None, savgol_window=7, savgol_order=3)\n",
    "\n",
    "\n",
    "plot_cluster_map(gdf_cities, clusters, cluster_colors, cluster_id_col='cluster_id', city_num_col='city_index', save_path_jpg=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef5db6",
   "metadata": {},
   "source": [
    "# Spatial autocorrelation, Moran's I (global), and spatial lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419049d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "%run Spatial_autocorrelation_Morans_I_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take vector field of one month\n",
    "vector_one_month= vectors_flow_map_cities['2013'][1]\n",
    "\n",
    "#calculate  spatial weights matrix here we use queen method\n",
    "wq= calculate_spatial_weights(gdf_cities, method='queen')\n",
    "\n",
    "#add vector sizes to gdf file as a column named 'vector_size'\n",
    "gdf_cities= add_vector_size_to_gdf(gdf_cities, vector_one_month)\n",
    "\n",
    "#calculate spatial lag and add it to gdf as column named 'spatial_lag'\n",
    "gdf_cities= calculate_spatial_lag(gdf_cities, 'vector_size', wq)\n",
    "\n",
    "#calculate Moran's I\n",
    "# put variable interested to calculate spatial lag and Moran'I - here is vector sizes\n",
    "y = gdf_cities.vector_size\n",
    "\n",
    "#calculate Moran's I\n",
    "moran_calculation = Moran(y, wq)\n",
    "print(moran_calculation.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9432894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmaps of vector sizes and spatial lag\n",
    "\"\"\"scheme='quantiles' gives the same number of cities in each range\"\"\"\n",
    "plot_variable_heatmap(gdf_cities, 'vector_size', title='vector_size', cmap='RdYlGn_r' , save_path=None)\n",
    "plot_variable_heatmap(gdf_cities, 'spatial_lag', title='spatial_lag', cmap='RdYlGn_r', save_path=None)\n",
    "\n",
    "\n",
    "#Plot Moran's I plots\n",
    "plot_moran_scatterplot(moran_calculation, save_path=None)\n",
    "plot_moran_simulation_distribution(moran_calculation, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146db0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Moran's I for all monthly vector fields for four years and plot them \n",
    "\n",
    "#calculate  spatial weights matrix here we use queen method\n",
    "wq= calculate_spatial_weights(gdf_b, method='queen')\n",
    "\n",
    "moran_months={}\n",
    "for ye in years:\n",
    "    moran_months[ye]={}\n",
    "    for mo in range(mw):\n",
    "        vector_one_month= vectors_flow_map[ye][mo+1]\n",
    "        \n",
    "        #add vector sizes to gdf file as a column named 'vector_size'\n",
    "        gdf_b= add_vector_size_to_gdf(gdf_b, vector_one_month)\n",
    "\n",
    "        #calculate spatial lag and add it to gdf as column named 'spatial_lag'\n",
    "        gdf_b= calculate_spatial_lag(gdf_b, 'vector_size', wq)\n",
    "\n",
    "        #calculate Moran's I\n",
    "        # put variable interested to calculate spatial lag and Moran'I - here is vector sizes\n",
    "        y = gdf_b.vector_size\n",
    "\n",
    "        #calculate Moran's I\n",
    "        moran_calculation = Moran(y, wq)\n",
    "        \n",
    "        I= moran_calculation.I\n",
    "        moran_months[ye][mo+1]=I\n",
    "    \n",
    "#plot the heatmap of the moran's values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the heatmap of the moran's values\n",
    "months = list(range(1,13))\n",
    "years = ['2013', '2014', '2015', '2016']\n",
    "\n",
    "# Create the data array\n",
    "moran_array = np.array([[moran_months[year][month] for month in months] for year in years])\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Plot heatmap\n",
    "sns.set(font_scale=1)\n",
    "ax.set_facecolor('white')\n",
    "heatmap = sns.heatmap(\n",
    "    moran_array,\n",
    "    cmap='RdBu_r',\n",
    "    linewidth=0.8,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cbar=True,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Customize ticks and labels\n",
    "ax.set_xticklabels(range(1, 13))\n",
    "ax.set_yticklabels(years)\n",
    "ax.set_title(\"Moran's Index\", fontsize=20)\n",
    "ax.set_xlabel('Month', fontsize=14)\n",
    "ax.set_ylabel('Year', fontsize=14)\n",
    "\n",
    "# Set tick font sizes\n",
    "ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Add label to colorbar\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.set_label('Moran\\'s I Value', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e809a",
   "metadata": {},
   "source": [
    "# Analysis using microregions as initial divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read shapefile of the micro-regions\n",
    "gdf_microregions= gpd.read_file('polygon_shapefiles/micro_regions/31MI2500G.shp')\n",
    "#make a column including region number\n",
    "gdf_microregions['microregions_index'] = gdf_microregions.index\n",
    "\n",
    "\n",
    "#make the united boundary of the whole region using one of the shapfile\n",
    "gdf_boundary= gdf_microregions.dissolve()\n",
    "\n",
    "\n",
    "### coordinates is a n by 2 array where the i'th entry is the coordinates of the i'th city [x, y]\n",
    "with open(\"Coordinates/Coordinates_mics.pkl\", \"rb\") as f:\n",
    "    coordinates_microregions= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6dc13",
   "metadata": {},
   "source": [
    "## Paramethers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313999be",
   "metadata": {},
   "outputs": [],
   "source": [
    "MW = 12 #number of months\n",
    "mw = 12\n",
    "number_cities= len(gdf_cities)\n",
    "num_microregions= len(gdf_microregions)\n",
    "\n",
    "years=['2013', '2014', '2015', '2016']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fac38d",
   "metadata": {},
   "source": [
    "## Generate vector fields \n",
    "### for defined unites (microregions) from OD matrices and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "%run Generate_vector_fields.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2980b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_flow_map_microregions={}\n",
    "for y in years:\n",
    "    vectors_flow_map_microregions[y]= {}\n",
    "    for m in range(2):\n",
    "        with open( \"OD_matrices/OD_matrices_microregions/OD_mics_{}\".format(y) + \"_{}.pkl\".format(m+1), \"rb\") as f:\n",
    "            adj_matrix = pickle.load(f)\n",
    "            \n",
    "            \n",
    "            methods= 'mean'\n",
    "            vecs= generate_vector_for_defined_regions(adj_matrix, gdf_boundary, coordinates_microregions, method)\n",
    "            \n",
    "            vectors_flow_map_microregions[y][m+1]= vecs\n",
    "            \n",
    "            #with open(\"generated_vector_fields/flow_map_microregions_{}\".format(y) + \"_{}.pkl\".format(m+1), \"wb\") as f:\n",
    "                #pickle.dump(vecs, f)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f1729",
   "metadata": {},
   "source": [
    "## read the vector fileds that have been generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_flow_map_microregions= {}\n",
    "for y in years:\n",
    "    vectors_flow_map_microregions[y]= {}\n",
    "    for m in range(mw):\n",
    "        with open(\"generated_vector_fields/flow_map_microregions_{}\".format(y) + \"_{}.pkl\".format(m+1), \"rb\") as f:\n",
    "            vecs = pickle.load(f)\n",
    "            vectors_flow_map_microregions[y][m+1]= vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106b7b3",
   "metadata": {},
   "source": [
    "## Entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2515df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "%run entropy_calculation_function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733483a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_dictionary_microregions = calculate_entropy(vectors_flow_map_microregions, years, num_microregions, mw=MW, n_label=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Find global min and max values for consistent color scaling**\n",
    "global_min = min(np.min(arr) for arr in entropy_dictionary_cities.values())\n",
    "global_max = max(np.max(arr) for arr in entropy_dictionary_cities.values())\n",
    "\n",
    "# Define colormap\n",
    "orig_map = plt.cm.get_cmap('RdBu').reversed()  # Reverse color map\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(1, len(years), figsize=(80, 20))\n",
    "\n",
    "# Loop through years and plot each map\n",
    "for i, y in enumerate(years):\n",
    "    gdf_microregions[f'entropy_{y}'] = gdf_microregions.apply(lambda row: add_ent(row, entropy_dictionary_cities[y]), axis=1)\n",
    "\n",
    "    ax = axes[i] if len(years) > 1 else axes  # Handle single subplot case\n",
    "\n",
    "    img = gdf_microregions.plot(column=f'entropy_{y}', cmap=orig_map, edgecolor='black', linewidth=0.1,\n",
    "                  ax=ax, legend=False, vmin=global_min, vmax=global_max)  # Apply global color scale\n",
    "\n",
    "    #ax.set_title('{}'.format(y))\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "\n",
    "# **Add a shared colorbar outside the figure**\n",
    "fig.subplots_adjust(right=0.85)  # Adjust space for colorbar\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])  # Position: [left, bottom, width, height]\n",
    "sm = plt.cm.ScalarMappable(cmap=orig_map, norm=plt.Normalize(vmin=global_min, vmax=global_max))\n",
    "fig.colorbar(sm, cax=cbar_ax, label=\"Entropy Value\")\n",
    "#plt.savefig('entropy_months_microregions_all_cbar_mean.jpg', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9df2ff",
   "metadata": {},
   "source": [
    "## calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7941eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "%run Cosine_similarity_function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all city vectors in order across all years\n",
    "vector_microregion_months_list = []\n",
    "\n",
    "for ye in years:\n",
    "    vector_microregion_months_list.append(np.stack(list(vectors_flow_map_microregions[ye].values()), axis=-1))\n",
    "\n",
    "# Concatenate along the month axis to create a (number_regions, 2, total_months) array\n",
    "vectors_microregion_months = np.concatenate(vector_microregion_months_list, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134671ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cosine similarity across the full 4-year period\n",
    "\n",
    "vector_months_microregion_cosine_similarity = calculate_cosine_sim(vectors_microregion_months)\n",
    "\n",
    "\n",
    "#calculate the distances between the cosine similarity values for the months for the regions\n",
    "distance_matrix= calculate_dtw_distance_matrix(vector_months_microregion_cosine_similarity)\n",
    "\n",
    "#cluster the regions(cities) based on the distance matrix\n",
    "#select how many clusters you are interested to get\n",
    "n_clusters=5\n",
    "clusters, kmedoids= perform_kmedoids_clustering(distance_matrix, n_clusters)\n",
    "\n",
    "#calculate the mean of the cosine similarity values of the regions(cities) in each clusters\n",
    "clusters_mean= calculate_cluster_means(vector_months_microregion_cosine_similarity, clusters)\n",
    "\n",
    "#plot clusters mean cosine similarity values for months and fit a curve\n",
    "#assign color for clusters\n",
    "cluster_colors = {\n",
    "    0: \"#0072b2\",\n",
    "    1: \"#009e73\",\n",
    "    2: \"#e69f00\",\n",
    "    3: '#d55e00',\n",
    "    4:'#cc79a7'\n",
    "}\n",
    "\n",
    "plot_combined_cluster_trends(clusters_mean, cluster_colors, save_path_svg=None, savgol_window=7, savgol_order=3)\n",
    "\n",
    "\n",
    "plot_cluster_map(gdf_microregions, clusters, cluster_colors, cluster_id_col='cluster_id', city_num_col='microregion_index', save_path_jpg=None)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
